{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fedfe9dc-37f1-42ce-99e2-83b2de64cd9a",
   "metadata": {},
   "source": [
    "# Dependency\n",
    "\n",
    "Following packages are required for inference (the specified version has been tested):\n",
    "```\n",
    "accelerate==0.33.0\n",
    "transformers==4.44.2\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "886a86f7-efe3-40b4-814a-7fa6b6da5dc3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\n",
      "Collecting accelerate==0.33.0\n",
      "  Downloading accelerate-0.33.0-py3-none-any.whl.metadata (18 kB)\n",
      "Collecting transformers==4.44.2\n",
      "  Downloading transformers-4.44.2-py3-none-any.whl.metadata (43 kB)\n",
      "Requirement already satisfied: numpy<2.0.0,>=1.17 in /usr/local/lib/python3.10/dist-packages (from accelerate==0.33.0) (1.24.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from accelerate==0.33.0) (23.2)\n",
      "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate==0.33.0) (6.0.0)\n",
      "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from accelerate==0.33.0) (6.0.2)\n",
      "Requirement already satisfied: torch>=1.10.0 in /usr/local/lib/python3.10/dist-packages (from accelerate==0.33.0) (2.5.0a0+872d972e41.nv24.8)\n",
      "Collecting huggingface-hub>=0.21.0 (from accelerate==0.33.0)\n",
      "  Downloading huggingface_hub-0.24.6-py3-none-any.whl.metadata (13 kB)\n",
      "Collecting safetensors>=0.3.1 (from accelerate==0.33.0)\n",
      "  Downloading safetensors-0.4.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.8 kB)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers==4.44.2) (3.15.4)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers==4.44.2) (2024.7.24)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers==4.44.2) (2.32.3)\n",
      "Collecting tokenizers<0.20,>=0.19 (from transformers==4.44.2)\n",
      "  Downloading tokenizers-0.19.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n",
      "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers==4.44.2) (4.66.5)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.21.0->accelerate==0.33.0) (2024.6.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.21.0->accelerate==0.33.0) (4.12.2)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate==0.33.0) (3.3)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate==0.33.0) (3.1.4)\n",
      "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate==0.33.0) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch>=1.10.0->accelerate==0.33.0) (1.3.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.44.2) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.44.2) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.44.2) (2.0.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.44.2) (2024.7.4)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.10.0->accelerate==0.33.0) (2.1.5)\n",
      "Downloading accelerate-0.33.0-py3-none-any.whl (315 kB)\n",
      "Downloading transformers-4.44.2-py3-none-any.whl (9.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.5/9.5 MB\u001b[0m \u001b[31m175.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading huggingface_hub-0.24.6-py3-none-any.whl (417 kB)\n",
      "Downloading safetensors-0.4.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (435 kB)\n",
      "Downloading tokenizers-0.19.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.6/3.6 MB\u001b[0m \u001b[31m311.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: safetensors, huggingface-hub, tokenizers, accelerate, transformers\n",
      "Successfully installed accelerate-0.33.0 huggingface-hub-0.24.6 safetensors-0.4.5 tokenizers-0.19.1 transformers-4.44.2\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable.It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install accelerate==0.33.0 transformers==4.44.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "03f872bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b61cdb23",
   "metadata": {},
   "source": [
    "# Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eec2c782-5209-4dd3-8bf5-42eec95c2882",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dc19e675-ac8d-4ac7-a29d-7ecd626c42fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 17/17 [00:20<00:00,  1.23s/it]\n"
     ]
    }
   ],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained( \n",
    "    \"microsoft/GRIN-MoE\",\n",
    "    device_map=\"auto\",  \n",
    "    torch_dtype=\"auto\",  \n",
    "    trust_remote_code=True,  \n",
    ") \n",
    "tokenizer = AutoTokenizer.from_pretrained(\"microsoft/GRIN-MoE\")\n",
    "pipe = pipeline( \n",
    "    \"text-generation\", \n",
    "    model=model, \n",
    "    tokenizer=tokenizer, \n",
    ") \n",
    "generation_args = { \n",
    "    \"max_new_tokens\": 2048, \n",
    "    \"return_full_text\": False, \n",
    "    \"do_sample\": False, \n",
    "} "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e70e6a3",
   "metadata": {},
   "source": [
    "# Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "93bf1515",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7f2f95b502b0>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.random.manual_seed(0) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3953e10e-67d0-4d85-a155-8b884474c7f4",
   "metadata": {},
   "source": [
    "## Example 1\n",
    "\n",
    "\n",
    "### Question\n",
    "Please complete the following single-choice question. The question has four options, and only one of them is correct. Select the option that is correct. \n",
    "\n",
    "Question: Given $\\cos(\\alpha + \\beta) = m$, and $\\tan(\\alpha) \\tan(\\beta) = 2$, we want to find $\\cos(\\alpha - \\beta) =$\n",
    "\n",
    "Option A: $-3m$\n",
    "\n",
    "Option B: $-\\frac{m}{3}$\n",
    "\n",
    "Option C: $\\frac{m}{3}$\n",
    "\n",
    "Option D: $3m$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20480447-bd3f-4dbc-8601-78b93e758877",
   "metadata": {},
   "source": [
    "### Inference\n",
    "\n",
    "As an aligned model, GRIN MoE is best suited for prompts using the chat format as follows:\n",
    "\n",
    "```\n",
    "<|system|>\n",
    "You are a helpful assistant.<|end|>\n",
    "<|user|>\n",
    "$USER_MESAGE<|end|>\n",
    "<|assistant|>\n",
    "```\n",
    "\n",
    "The following code will format the above question in this format. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6a0fefbe-9aa1-40dc-b5fe-d067572094d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The `seen_tokens` attribute is deprecated and will be removed in v4.41. Use the `cache_position` model input instead.\n"
     ]
    }
   ],
   "source": [
    "message = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful AI assistant.\"}, \n",
    "    {\"role\": \"user\", \"content\": \"Please complete the following single-choice question. The question has four options, and only one of them is correct. Select the option that is correct. \\n\\nQuestion: Given $\\\\cos(\\\\alpha + \\\\beta) = m$, and $\\\\tan(\\\\alpha) \\\\tan(\\\\beta) = 2$, we want to find $\\\\cos(\\\\alpha - \\\\beta) =$\\n\\nOption A: $-3m$\\nOption B: $-\\\\frac{m}{3}$\\nOption C: $\\\\frac{m}{3}$\\nOption D: $3m$\"}\n",
    "]\n",
    "\n",
    "output = pipe(message, **generation_args) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e906729b-7d30-4c8f-a48b-182bce54adb2",
   "metadata": {},
   "source": [
    "### Result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "16354f52-376d-435e-b1db-8379d004f719",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------Correct Answer RESPONSE-----------\n",
      "The correct answer of this question is A, -3m\n",
      "-----------Model RESPONSE-----------\n",
      " To find $\\cos(\\alpha - \\beta)$, we can use the cosine difference identity:\n",
      "\n",
      "$\\cos(\\alpha - \\beta) = \\cos(\\alpha)\\cos(\\beta) + \\sin(\\alpha)\\sin(\\beta)$\n",
      "\n",
      "We know that $\\cos(\\alpha + \\beta) = m$, so we can use the cosine sum identity:\n",
      "\n",
      "$\\cos(\\alpha + \\beta) = \\cos(\\alpha)\\cos(\\beta) - \\sin(\\alpha)\\sin(\\beta) = m$\n",
      "\n",
      "Now, we can use the given information $\\tan(\\alpha) \\tan(\\beta) = 2$. We know that $\\tan(\\alpha) = \\frac{\\sin(\\alpha)}{\\cos(\\alpha)}$ and $\\tan(\\beta) = \\frac{\\sin(\\beta)}{\\cos(\\beta)}$. So,\n",
      "\n",
      "$\\frac{\\sin(\\alpha)}{\\cos(\\alpha)} \\cdot \\frac{\\sin(\\beta)}{\\cos(\\beta)} = 2$\n",
      "\n",
      "$\\sin(\\alpha)\\sin(\\beta) = 2\\cos(\\alpha)\\cos(\\beta)$\n",
      "\n",
      "Now, we can substitute this into the cosine difference identity:\n",
      "\n",
      "$\\cos(\\alpha - \\beta) = \\cos(\\alpha)\\cos(\\beta) + 2\\cos(\\alpha)\\cos(\\beta) = 3\\cos(\\alpha)\\cos(\\beta)$\n",
      "\n",
      "We also know that $\\cos(\\alpha + \\beta) = \\cos(\\alpha)\\cos(\\beta) - \\sin(\\alpha)\\sin(\\beta) = m$. So,\n",
      "\n",
      "$m = \\cos(\\alpha)\\cos(\\beta) - 2\\cos(\\alpha)\\cos(\\beta) = -\\cos(\\alpha)\\cos(\\beta)$\n",
      "\n",
      "Now, we can substitute this into the expression for $\\cos(\\alpha - \\beta)$:\n",
      "\n",
      "$\\cos(\\alpha - \\beta) = 3(-\\cos(\\alpha)\\cos(\\beta)) = -3m$\n",
      "\n",
      "So, the correct option is:\n",
      "\n",
      "Option A: $-3m$\n"
     ]
    }
   ],
   "source": [
    "print(\"-----------Correct Answer RESPONSE-----------\")\n",
    "print(\"The correct answer of this question is A, -3m\")\n",
    "print(\"-----------Model RESPONSE-----------\")\n",
    "print(output[0][\"generated_text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6b2bd88",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9a9c6753",
   "metadata": {},
   "source": [
    "## Example 2\n",
    "\n",
    "\n",
    "### Question\n",
    "If the tangent line to the curve $y = e^x + x$ at the point $(0, 1)$ is also the tangent line to the curve $y = ln(x+1) + a$ at some point, find the value of the constant $a$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3286a4a8-f3ad-4b6e-8003-5941c192d9be",
   "metadata": {},
   "source": [
    "### Inference\n",
    "\n",
    "As an aligned model, GRIN MoE is best suited for prompts using the chat format as follows:\n",
    "\n",
    "```\n",
    "<|system|>\n",
    "You are a helpful assistant.<|end|>\n",
    "<|user|>\n",
    "$USER_MESAGE<|end|>\n",
    "<|assistant|>\n",
    "```\n",
    "\n",
    "The following code will format the above question in this format. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "79f916c1-317f-4960-9bec-968b8fffe61d",
   "metadata": {},
   "outputs": [],
   "source": [
    "message = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful AI assistant.\"}, \n",
    "    {\"role\": \"user\", \"content\": \"If the tangent line to the curve $y = e^x + x$ at the point $(0, 1)$ is also the tangent line to the curve $y = ln(x+1) + a$ at some point, find the value of the constant $a$.\"}\n",
    "]\n",
    "\n",
    "output = pipe(message, **generation_args) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c4c2a0b-a794-4448-8c4b-6c85af14854b",
   "metadata": {},
   "source": [
    "### Result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "279faa1f-74af-45bf-a0d5-49b80e1f23af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------Correct Answer RESPONSE-----------\n",
      "The correct answer of this question is ln(2)\n",
      "-----------Model RESPONSE-----------\n",
      " First, let's find the slope of the tangent line to the curve $y = e^x + x$ at the point $(0, 1)$. To do this, we need to find the derivative of the function $y = e^x + x$ with respect to $x$.\n",
      "\n",
      "The derivative of $e^x$ with respect to $x$ is $e^x$, and the derivative of $x$ with respect to $x$ is $1$. So, the derivative of $y = e^x + x$ is:\n",
      "\n",
      "$y' = e^x + 1$\n",
      "\n",
      "Now, we can find the slope of the tangent line at the point $(0, 1)$ by plugging in $x = 0$ into the derivative:\n",
      "\n",
      "$y'(0) = e^0 + 1 = 1 + 1 = 2$\n",
      "\n",
      "So, the slope of the tangent line to the curve $y = e^x + x$ at the point $(0, 1)$ is $2$.\n",
      "\n",
      "Now, let's find the equation of the tangent line to the curve $y = e^x + x$ at the point $(0, 1)$. We can use the point-slope form of a linear equation:\n",
      "\n",
      "$y - y_1 = m(x - x_1)$\n",
      "\n",
      "where $(x_1, y_1)$ is the point $(0, 1)$ and $m$ is the slope $2$. Plugging in these values, we get:\n",
      "\n",
      "$y - 1 = 2(x - 0)$\n",
      "\n",
      "Simplifying, we get the equation of the tangent line:\n",
      "\n",
      "$y = 2x + 1$\n",
      "\n",
      "Now, we need to find the point where this tangent line is also tangent to the curve $y = ln(x+1) + a$. To do this, we need to find the derivative of the function $y = ln(x+1) + a$ with respect to $x$.\n",
      "\n",
      "The derivative of $ln(x+1)$ with respect to $x$ is $\\frac{1}{x+1}$, and the derivative of $a$ with respect to $x$ is $0$. So, the derivative of $y = ln(x+1) + a$ is:\n",
      "\n",
      "$y' = \\frac{1}{x+1}$\n",
      "\n",
      "Now, we need to find the point where the slope of the tangent line to the curve $y = ln(x+1) + a$ is equal to the slope of the tangent line to the curve $y = e^x + x$ at the point $(0, 1)$, which is $2$. So, we need to solve the equation:\n",
      "\n",
      "$\\frac{1}{x+1} = 2$\n",
      "\n",
      "Multiplying both sides by $x+1$, we get:\n",
      "\n",
      "$1 = 2(x+1)$\n",
      "\n",
      "Simplifying, we get:\n",
      "\n",
      "$1 = 2x + 2$\n",
      "\n",
      "Subtracting $2$ from both sides, we get:\n",
      "\n",
      "$-1 = 2x$\n",
      "\n",
      "Dividing both sides by $2$, we get:\n",
      "\n",
      "$x = -\\frac{1}{2}$\n",
      "\n",
      "Now, we can find the $y$-coordinate of the point where the tangent line to the curve $y = ln(x+1) + a$ is tangent to the curve $y = e^x + x$ at the point $(0, 1)$. Plugging in $x = -\\frac{1}{2}$ into the equation of the curve $y = ln(x+1) + a$, we get:\n",
      "\n",
      "$y = ln\\left(-\\frac{1}{2}+1\\right) + a = ln\\left(\\frac{1}{2}\\right) + a$\n",
      "\n",
      "Now, we know that the tangent line to the curve $y = ln(x+1) + a$ at the point $\\left(-\\frac{1}{2}, ln\\left(\\frac{1}{2}\\right) + a\\right)$ has the same equation as the tangent line to the curve $y = e^x + x$ at the point $(0, 1)$, which is $y = 2x + 1$. Plugging in $x = -\\frac{1}{2}$ and $y = ln\\left(\\frac{1}{2}\\right) + a$ into this equation, we get:\n",
      "\n",
      "$ln\\left(\\frac{1}{2}\\right) + a = 2\\left(-\\frac{1}{2}\\right) + 1$\n",
      "\n",
      "Simplifying, we get:\n",
      "\n",
      "$ln\\left(\\frac{1}{2}\\right) + a = -1 + 1$\n",
      "\n",
      "$ln\\left(\\frac{1}{2}\\right) + a = 0$\n",
      "\n",
      "Now, we can solve for $a$:\n",
      "\n",
      "$a = -ln\\left(\\frac{1}{2}\\right)$\n",
      "\n",
      "Using the property of logarithms that $ln\\left(\\frac{1}{x}\\right) = -ln(x)$, we can rewrite this as:\n",
      "\n",
      "$a = ln(2)$\n",
      "\n",
      "So, the value of the constant $a$ is $ln(2)$.\n"
     ]
    }
   ],
   "source": [
    "print(\"-----------Correct Answer RESPONSE-----------\")\n",
    "print(\"The correct answer of this question is ln(2)\")\n",
    "print(\"-----------Model RESPONSE-----------\")\n",
    "print(output[0][\"generated_text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "293a455c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6a3a98ed-c251-465a-849d-ca8d02b65eff",
   "metadata": {},
   "source": [
    "## Interactive Chat \n",
    "\n",
    "Run the following command to interact with GRIN-MoE in the chat manner. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a482fc64-d135-4c55-8fdb-5411063ea482",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------user-------------\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "user input (enter <|stop|> to stop):  Hello, I want to ask you some math questions. can you do that?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------user-------------\n",
      "Hello, I want to ask you some math questions. can you do that?\n",
      "------------assistant-------------\n",
      " Of course! I'd be happy to help you with your math questions. Please go ahead and ask.\n",
      "------------user-------------\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "user input (enter <|stop|> to stop):  Solve the equation: $\\frac{2x + 3}{x - 1} = \\frac{x + 2}{x + 1}$.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------user-------------\n",
      "Solve the equation: $\\frac{2x + 3}{x - 1} = \\frac{x + 2}{x + 1}$.\n",
      "------------assistant-------------\n",
      " Sure, I can help you solve the equation. To solve the equation $\\frac{2x + 3}{x - 1} = \\frac{x + 2}{x + 1}$, we can start by cross-multiplying:\n",
      "\n",
      "$(2x + 3)(x + 1) = (x + 2)(x - 1)$\n",
      "\n",
      "Now, let's expand both sides:\n",
      "\n",
      "$2x^2 + 2x + 3x + 3 = x^2 + 2x - x - 2$\n",
      "\n",
      "Combine like terms:\n",
      "\n",
      "$2x^2 + 5x + 3 = x^2 + x - 2$\n",
      "\n",
      "Now, let's move all terms to one side to set the equation to zero:\n",
      "\n",
      "$2x^2 - x^2 + 5x - x + 3 + 2 = 0$\n",
      "\n",
      "Simplify:\n",
      "\n",
      "$x^2 + 4x + 5 = 0$\n",
      "\n",
      "This is a quadratic equation. We can solve it using the quadratic formula:\n",
      "\n",
      "$x = \\frac{-b \\pm \\sqrt{b^2 - 4ac}}{2a}$\n",
      "\n",
      "In this case, $a = 1$, $b = 4$, and $c = 5$. Plugging these values into the formula, we get:\n",
      "\n",
      "$x = \\frac{-4 \\pm \\sqrt{4^2 - 4(1)(5)}}{2(1)}$\n",
      "\n",
      "$x = \\frac{-4 \\pm \\sqrt{16 - 20}}{2}$\n",
      "\n",
      "Since the discriminant ($b^2 - 4ac$) is negative, there are no real solutions for this equation. The solutions are complex numbers:\n",
      "\n",
      "$x = \\frac{-4 \\pm \\sqrt{-4}}{2}$\n",
      "\n",
      "$x = \\frac{-4 \\pm 2i}{2}$\n",
      "\n",
      "$x = -2 \\pm i$\n",
      "\n",
      "So, the solutions to the equation are $x = -2 + i$ and $x = -2 - i$.\n",
      "------------user-------------\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "user input (enter <|stop|> to stop):  Sally (a girl) has 3 brothers. Each brother has 2 sisters. How many sisters does Sally have?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------user-------------\n",
      "Sally (a girl) has 3 brothers. Each brother has 2 sisters. How many sisters does Sally have?\n",
      "------------assistant-------------\n",
      " Sally has 1 sister.\n",
      "\n",
      "Here's the explanation:\n",
      "\n",
      "Sally has 3 brothers. Each brother has 2 sisters, but since they all share the same sisters, we don't count them multiple times. We know Sally is one sister, so there must be one more sister to make the count of 2 sisters for each brother. Therefore, Sally has 1 sister.\n",
      "------------user-------------\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "user input (enter <|stop|> to stop):  Can you help me revise the abstract as below? Focus on any potential grammar errors. Thanks!  Mixture-of-Experts (MoE) models scale more effectively than dense models due to sparse computation through expert routing, selectively activating only a small subset of expert modules. However, sparse computation challenges traditional training practices, as discrete expert routing hinders standard backpropagation and thus gradient-based optimization, which are the cornerstone of deep learning. To better pursue the scaling power of MoE, we introduce GRIN (GRadient-INformed MoE training), which incorporates sparse gradient estimation for expert routing and configures model parallelism to avoid token dropping. Applying GRIN to autoregressive language modeling, we develope a top-2 16x3.8B MoE model. Our model, with only 6.6B activated parameters, outperforms a 7B dense model and matches the performance of a 14B dense model trained on the same data. Extensive evaluations across diverse tasks demonstrate the potential of GRIN to significantly enhance MoE efficacy, achieving 79.4 on MMLU, 83.7 on HellaSwag, 74.4 on HumanEval, and 58.9 on MATH.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------user-------------\n",
      "Can you help me revise the abstract as below? Focus on any potential grammar errors. Thanks!  Mixture-of-Experts (MoE) models scale more effectively than dense models due to sparse computation through expert routing, selectively activating only a small subset of expert modules. However, sparse computation challenges traditional training practices, as discrete expert routing hinders standard backpropagation and thus gradient-based optimization, which are the cornerstone of deep learning. To better pursue the scaling power of MoE, we introduce GRIN (GRadient-INformed MoE training), which incorporates sparse gradient estimation for expert routing and configures model parallelism to avoid token dropping. Applying GRIN to autoregressive language modeling, we develope a top-2 16x3.8B MoE model. Our model, with only 6.6B activated parameters, outperforms a 7B dense model and matches the performance of a 14B dense model trained on the same data. Extensive evaluations across diverse tasks demonstrate the potential of GRIN to significantly enhance MoE efficacy, achieving 79.4 on MMLU, 83.7 on HellaSwag, 74.4 on HumanEval, and 58.9 on MATH.\n",
      "------------assistant-------------\n",
      " Mixture-of-Experts (MoE) models scale more effectively than dense models due to sparse computation through expert routing, selectively activating only a small subset of expert modules. However, sparse computation challenges traditional training practices, as discrete expert routing hinders standard backpropagation and thus gradient-based optimization, which are the cornerstones of deep learning. To better harness the scaling power of MoE, we introduce GRIN (GRadient-INformed MoE training), which incorporates sparse gradient estimation for expert routing and configures model parallelism to avoid token dropping. Applying GRIN to autoregressive language modeling, we develop a top-2 16x3.8B MoE model. Our model, with only 6.6B activated parameters, outperforms a 7B dense model and matches the performance of a 14B dense model trained on the same data. Extensive evaluations across diverse tasks demonstrate the potential of GRIN to significantly enhance MoE efficacy, achieving 79.4 on MMLU, 83.7 on HellaSwag, 74.4 on HumanEval, and 58.9 on MATH.\n",
      "\n",
      "The abstract is well-written and mostly error-free. However, I have made a few minor adjustments for clarity and flow:\n",
      "\n",
      "1. Changed \"pursue\" to \"harness\" in the second paragraph.\n",
      "2. Changed \"develope\" to \"develop\" in the third paragraph.\n",
      "\n",
      "The revised abstract is as follows:\n",
      "\n",
      "Mixture-of-Experts (MoE) models scale more effectively than dense models due to sparse computation through expert routing, selectively activating only a small subset of expert modules. However, sparse computation challenges traditional training practices, as discrete expert routing hinders standard backpropagation and thus gradient-based optimization, which are the cornerstones of deep learning. To better harness the scaling power of MoE, we introduce GRIN (GRadient-INformed MoE training), which incorporates sparse gradient estimation for expert routing and configures model parallelism to avoid token dropping. Applying GRIN to autoregressive language modeling, we develop a top-2 16x3.8B MoE model. Our model, with only 6.6B activated parameters, outperforms a 7B dense model and matches the performance of a 14B dense model trained on the same data. Extensive evaluations across diverse tasks demonstrate the potential of GRIN to significantly enhance MoE efficacy, achieving 79.4 on MMLU, 83.7 on HellaSwag, 74.4 on HumanEval, and 58.9 on MATH.\n",
      "------------user-------------\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "user input (enter <|stop|> to stop):  i feel pursue is a better word.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------user-------------\n",
      "i feel pursue is a better word.\n",
      "------------assistant-------------\n",
      " Certainly! Here's the revised abstract with the suggested change:\n",
      "\n",
      "Mixture-of-Experts (MoE) models scale more effectively than dense models due to sparse computation through expert routing, selectively activating only a small subset of expert modules. However, sparse computation challenges traditional training practices, as discrete expert routing hinders standard backpropagation and thus gradient-based optimization, which are the cornerstone of deep learning. To better **pursue** the scaling power of MoE, we introduce GRIN (GRadient-INformed MoE training), which incorporates sparse gradient estimation for expert routing and configures model parallelism to avoid token dropping. Applying GRIN to autoregressive language modeling, we develop a top-2 16x3.8B MoE model. Our model, with only 6.6B activated parameters, outperforms a 7B dense model and matches the performance of a 14B dense model trained on the same data. Extensive evaluations across diverse tasks demonstrate the potential of GRIN to significantly enhance MoE efficacy, achieving 79.4 on MMLU, 83.7 on HellaSwag, 74.4 on HumanEval, and 58.9 on MATH.\n",
      "------------user-------------\n"
     ]
    }
   ],
   "source": [
    "message = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful AI assistant.\"}, \n",
    "]\n",
    "\n",
    "while True:\n",
    "    print('------------user-------------')\n",
    "    content = input('user input (enter <|stop|> to stop): ')\n",
    "    print('------------user-------------')\n",
    "    if '<|stop|>' in content:\n",
    "       break \n",
    "    print(content)\n",
    "    message.append({\"role\": \"user\", \"content\": content})\n",
    "    output = pipe(message, **generation_args)\n",
    "    print('------------assistant-------------')\n",
    "    print(output[0][\"generated_text\"])\n",
    "    message.append({\"role\": \"assistant\", \"content\": content})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f479b6ca-5b26-4ae4-b3f2-c0c61df99add",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e754197-2efe-4154-8a59-b9f980733a9d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
